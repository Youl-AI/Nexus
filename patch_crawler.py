import requests
from bs4 import BeautifulSoup
import os
import re

# 1. ë°ì´í„° ì €ì¥ í´ë”
DATA_DIR = "data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def clean_filename(title):
    """
    1. íŒŒì¼ëª…ìœ¼ë¡œ ëª» ì“°ëŠ” íŠ¹ìˆ˜ë¬¸ì ì œê±°
    2. ì•ë’¤ ê³µë°± ì œê±°
    3. ì¤‘ê°„ ê³µë°±ì„ ì–¸ë”ë°”(_)ë¡œ ë³€ê²½
    """
    # ìœˆë„ìš° íŒŒì¼ëª… ê¸ˆì§€ ë¬¸ì ì œê±°
    cleaned = re.sub(r'[\\/*?:"<>|]', "", title)
    
    # ì–‘ìª½ ê³µë°± ì œê±° í›„, ì¤‘ê°„ ê³µë°±(ë„ì–´ì“°ê¸°, íƒ­ ë“±)ì„ ëª¨ë‘ ì–¸ë”ë°”(_)ë¡œ ë³€ê²½
    # \s+ ëŠ” "í•˜ë‚˜ ì´ìƒì˜ ê³µë°±"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    cleaned = re.sub(r'\s+', '_', cleaned.strip())
    
    return cleaned

def crawl_url(target_url):
    print(f"ğŸ•¸ï¸ í¬ë¡¤ë§ ì‹œì‘: {target_url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(target_url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì œëª© ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ untitled)
        raw_title = soup.title.string if soup.title else "untitled_page"
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì „ ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "footer", "header", "iframe"]):
            script.decompose()
            
        text_content = soup.get_text(separator="\n", strip=True)

        # â˜… íŒŒì¼ëª… ë³€í™˜ ì ìš©
        safe_title = clean_filename(raw_title)
        
        # íŒŒì¼ ì €ì¥
        file_path = os.path.join(DATA_DIR, f"{safe_title}.txt")
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"URL: {target_url}\n")
            f.write(f"Original Title: {raw_title}\n")
            f.write("-" * 30 + "\n")
            f.write(text_content)
            
        print(f"âœ… ì €ì¥ ì™„ë£Œ! -> {file_path}")
        print(f"   (íŒŒì¼ëª… ì˜ˆì‹œ: 'ë¦¬ê·¸_ì˜¤ë¸Œ_ë ˆì „ë“œ_íŒ¨ì¹˜ë…¸íŠ¸.txt')")

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("=== Nexus Web Crawler V2 (íŒŒì¼ëª… ìë™ ì •ë¦¬) ===")
    print("URLì„ ì…ë ¥í•˜ë©´ ë‚´ìš©ì„ 'ì œëª©_ê³µë°±_ì œê±°.txt'ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
    print("('q' ì…ë ¥ ì‹œ ì¢…ë£Œ)")
    
    while True:
        url = input("\nURL ì…ë ¥ >> ")
        
        if url.lower() in ['q', 'quit', 'exit']:
            break
            
        if not url.startswith("http"):
            print("âš ï¸ URLì€ http/httpsë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
            continue
            
        crawl_url(url)
