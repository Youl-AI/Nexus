import requests
from bs4 import BeautifulSoup
import os
import re
import time

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
DATA_DIR = "data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# 1. ìˆ˜ì§‘í•  ìœ„í‚¤ í˜ì´ì§€ ëª©ë¡ (ë” í•„ìš”í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì—¬ê¸°ì— ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤)
TARGET_URLS = {
    "ë¯¸ë‹ˆì–¸ (Minions)": "https://leagueoflegends.fandom.com/wiki/Minion_(League_of_Legends)",
    "ì •ê¸€ë§ (Jungling)": "https://leagueoflegends.fandom.com/wiki/Jungling",
    "í¬íƒ‘ (Turrets)": "https://leagueoflegends.fandom.com/wiki/Turret",
    "ì–µì œê¸° (Inhibitor)": "https://leagueoflegends.fandom.com/wiki/Inhibitor",
    "ë°©ì–´êµ¬ ê´€í†µë ¥ (Armor Penetration)": "https://leagueoflegends.fandom.com/wiki/Armor_penetration",
    "ë§ˆë²• ê´€í†µë ¥ (Magic Penetration)": "https://leagueoflegends.fandom.com/wiki/Magic_penetration",
    "ìŠ¤í‚¬ ê°€ì† (Ability Haste)": "https://leagueoflegends.fandom.com/wiki/Ability_Haste",
    "ì´ë™ ì†ë„ (Movement Speed)": "https://leagueoflegends.fandom.com/wiki/Movement_speed",
    "ëª¬ìŠ¤í„° (Monsters)": "https://leagueoflegends.fandom.com/wiki/Monster"
}

def clean_wiki_text(soup):
    """
    Fandom ìœ„í‚¤ì˜ ì§€ì €ë¶„í•œ ìš”ì†Œ(ê´‘ê³ , ë„¤ë¹„ê²Œì´ì…˜, í¸ì§‘ ë²„íŠ¼)ë¥¼ ì œê±°í•˜ê³  
    ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (Fandom ìœ„í‚¤ì˜ ë³¸ë¬¸ í´ë˜ìŠ¤)
    content = soup.find('div', {'class': 'mw-parser-output'})
    
    if not content:
        return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê´‘ê³ , í‘œì˜ ë¶ˆí•„ìš”í•œ í–‰, ì£¼ì„ ë“±)
    for tag in content(["script", "style", "nav", "figure", "aside", "noscript"]):
        tag.decompose()
        
    # 'í¸ì§‘' ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±° ([Edit])
    for tag in content.find_all(class_="mw-editsection"):
        tag.decompose()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ê³µë°± ì •ë¦¬
    text = content.get_text(separator="\n")
    
    # ë„ˆë¬´ ë§ì€ ë¹ˆ ì¤„ ì œê±° (3ì¤„ ì´ìƒ ë¹ˆ ì¤„ì€ 2ì¤„ë¡œ)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

def fetch_mechanics():
    print("ğŸ“š LoL ê²Œì„ ë©”ì»¤ë‹ˆì¦˜(Wiki) ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    save_path = os.path.join(DATA_DIR, "base_lol_mechanics.txt")
    
    # í—¤ë” ì‘ì„± (ë¸Œë¼ìš°ì €ì¸ ì²™ ì†ì´ê¸°)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    with open(save_path, "w", encoding="utf-8") as f:
        f.write("=== [LoL ê²Œì„ ë©”ì»¤ë‹ˆì¦˜ ë° ê³µì‹ ëª¨ìŒ] ===\n")
        f.write("ì¶œì²˜: League of Legends Fandom Wiki\n")
        f.write("ì´ ë°ì´í„°ëŠ” ê²Œì„ì˜ ê·œì¹™, ê³µì‹, AI(ë¯¸ë‹ˆì–¸/í¬íƒ‘) í–‰ë™ íŒ¨í„´ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n\n")

        total = len(TARGET_URLS)
        count = 0

        for title, url in TARGET_URLS.items():
            count += 1
            print(f"[{count}/{total}] ğŸ•¸ï¸ ìˆ˜ì§‘ ì¤‘: {title}...")
            
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                cleaned_text = clean_wiki_text(soup)
                
                # íŒŒì¼ì— ê¸°ë¡
                f.write(f"\n{'='*40}\n")
                f.write(f"## {title}\n")
                f.write(f"ì›ë³¸ ë§í¬: {url}\n")
                f.write(f"{'='*40}\n\n")
                f.write(cleaned_text)
                f.write("\n\n")
                
                print(f"   âœ… ì™„ë£Œ ({len(cleaned_text)}ì)")
                
                # ë„ˆë¬´ ë¹ ë¥´ê²Œ ìš”ì²­í•˜ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ 1ì´ˆ íœ´ì‹
                time.sleep(1)

            except Exception as e:
                print(f"   âŒ ì‹¤íŒ¨: {e}")
                f.write(f"\n## {title} (ìˆ˜ì§‘ ì‹¤íŒ¨)\nì—ëŸ¬: {e}\n\n")

    print(f"\nğŸ‰ ëª¨ë“  ë©”ì»¤ë‹ˆì¦˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}")
    print("   ì´ì œ Nexusê°€ 'ë°©ê´€ 30%ë©´ ë°©ì–´ë ¥ ì–¼ë§ˆ ë¬´ì‹œí•´?' ê°™ì€ ì§ˆë¬¸ì„ ì´í•´í•©ë‹ˆë‹¤!")

if __name__ == "__main__":
    fetch_mechanics()
